# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQcHOBuoaoqHRC31UTdIivhPsaEg3qEl
"""

from google.colab import drive

# Unmount the drive first
drive.flush_and_unmount()
print('Drive unmounted')

# Remove existing files from the mountpoint if it exists
import os
if os.path.exists('/content/drive'):
  !rm -rf '/content/drive'  # Use with caution! This permanently deletes all files in the directory.
  print('Files removed from mountpoint')

# Remount the drive
drive.mount('/content/drive')
print('Drive mounted')

# Libraries
import IPython.display as ipd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import preprocessing

# Read data
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv', index_col='song_name')

# Extract labels
genres = data[['genre']]

# Drop labels from original dataframe
data = data.drop(columns=['length','genre'])
data.head()

# Scale the data
data_scaled=preprocessing.scale(data)
print('Scaled data type:', type(data_scaled))

"""Cosine Similarity"""

# Cosine similarity
similarity = cosine_similarity(data_scaled)
print("Similarity shape:", similarity.shape)

sim_df_labels = pd.DataFrame(similarity)
sim_df_names = sim_df_labels.set_index(genres.index)
sim_df_names.columns = genres.index

sim_df_names.head()

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
def find_similar_songs(name):
    # Find songs most similar to another song
    series = sim_df_names[name].sort_values(ascending = False)

    # Remove cosine similarity == 1 (songs will always have the best match with themselves)
    series = series.drop(name)

    name = name.replace('.wav', '')

    # Display the 5 top matches
    print("\n*******\nSimilar songs to ", name)
    series.index = series.index.str.replace('.wav', '')
    print(series.head(5))
    print("*******")

find_similar_songs('Radiohead - Creep.wav')

find_similar_songs('Bach - Air - Best-of Classical Music.wav')

"""Nueral Network"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense
from scikeras.wrappers import KerasRegressor
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, scaled_data, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define a function to create the Keras model, required for KerasRegressor
def create_model(optimizer='adam', init='uniform', neurons1=128, neurons2=64, neurons3=32):
    model = Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(neurons1, activation='relu', kernel_initializer=init),
        Dense(neurons2, activation='relu', kernel_initializer=init),
        Dense(neurons3, activation='relu', kernel_initializer=init),
        Dense(X_train.shape[1], activation='linear')
    ])
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Wrapping the model using KerasRegressor
model = KerasRegressor(model=create_model, verbose=0)

# Hyperparameters grid
param_dist = {
    'batch_size': [16, 32, 64],
    'epochs': [50, 100, 150],
    'model__optimizer': ['adam', 'rmsprop'],
    'model__init': ['uniform', 'normal'],
    'model__neurons1': [64, 128, 256],
    'model__neurons2': [32, 64, 128],
    'model__neurons3': [16, 32, 64]
}

# Scorer
scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Performing RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring=scorer, cv=3, verbose=2, random_state=42)
random_search_result = random_search.fit(X_train, y_train)

# Printing the best parameters
print(f"Best parameters: {random_search_result.best_params_}")

# Training the model with the best parameters
best_model = random_search_result.best_estimator_
history = best_model.fit(X_train, y_train, epochs=random_search_result.best_params_['epochs'], batch_size=random_search_result.best_params_['batch_size'], validation_data=(X_val, y_val))

# Function to recommend similar songs
def recommend(song_name, num_recommendations=5):
    if song_name in data['song_name'].values:
        song_index = data[data['song_name'] == song_name].index[0]
        song_features = numeric_data.iloc[song_index].values.reshape(1, -1)
        scaled_song_features = scaler.transform(song_features)

        # Predict similar songs
        predictions = best_model.predict(scaled_song_features)

        # Calculate cosine similarity
        similarities = cosine_similarity(scaled_data, predictions).flatten()
        recommended_indices = similarities.argsort()[-(num_recommendations+1):-1][::-1]

        # Normalize similarity scores
        similarity_scores = similarities[recommended_indices]

        # Get recommended song names
        recommended_songs = data.iloc[recommended_indices]['song_name'].values

        song_name = song_name.replace('.wav', '')
        # Display the recommended songs and their similarity scores
        print(f"*******\nSimilar songs to '{song_name}':")
        for i, (song, score) in enumerate(zip(recommended_songs, similarity_scores)):
            song = song.replace('.wav', '')
            print(f"{song.ljust(80)} {score:.6f}")
        print("*******")
    else:
        print(f"Song '{song_name}' not found in the dataset.")

recommend('Radiohead - Creep.wav')

recommend('Herbie Hancock  Watermelon man.wav')

"""KNN"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
import numpy as np

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, scaled_data, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Defining the KNN model
knn = KNeighborsRegressor()

# Defining the parameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}

# Initializing GridSearchCV with validation set
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train, y_train)

# Printing the best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Training the model with the best parameters
best_knn = grid_search.best_estimator_
best_knn.fit(X_train, y_train)

# Function to recommend similar songs
def recommend(song_name, num_recommendations=5):
    if song_name in data['song_name'].values:
        song_index = data[data['song_name'] == song_name].index[0]
        song_features = numeric_data.iloc[song_index].values.reshape(1, -1)
        scaled_song_features = scaler.transform(song_features)

        # Find the nearest neighbors
        distances, indices = best_knn.kneighbors(scaled_song_features)

        # Get recommended song names and scores
        recommended_indices = indices.flatten()[1:num_recommendations+1]
        recommended_songs = data.iloc[recommended_indices]['song_name'].values
        similarity_scores = distances.flatten()[1:num_recommendations+1]

        song_name = song_name.replace('.wav', '')
        # Display the recommended songs and their similarity scores
        print(f"*******\nSimilar songs to '{song_name}':")
        for i, (song, score) in enumerate(zip(recommended_songs, similarity_scores)):
            song = song.replace('.wav', '')
            print(f"{song.ljust(80)} {score:.6f}")
        print("*******")
    else:
        print(f"Song '{song_name}' not found in the dataset.")

recommend('Radiohead - Creep.wav')

"""Decision Tree"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, scaled_data, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Defining the Decision Tree model
dt = DecisionTreeRegressor()

# Defining the parameter grid
param_grid = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8]
}

# Initializing GridSearchCV with validation set
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train, y_train)

# Printing the best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Training the model with the best parameters
best_dt = grid_search.best_estimator_
best_dt.fit(X_train, y_train)

# Function to recommend similar songs
def recommend(song_name, num_recommendations=5):
    if song_name in data['song_name'].values:
        song_index = data[data['song_name'] == song_name].index[0]
        song_features = numeric_data.iloc[song_index].values.reshape(1, -1)
        scaled_song_features = scaler.transform(song_features)

        # Predict similar songs
        predictions = best_dt.predict(scaled_song_features)

        # Calculate cosine similarity
        similarities = cosine_similarity(scaled_data, predictions.reshape(1, -1)).flatten()
        recommended_indices = similarities.argsort()[-(num_recommendations+1):-1][::-1]

        # Normalize similarity scores
        similarity_scores = similarities[recommended_indices]

        # Get recommended song names
        recommended_songs = data.iloc[recommended_indices]['song_name'].values

        song_name = song_name.replace('.wav', '')
        # Display the recommended songs and their similarity scores
        print(f"*******\nSimilar songs to '{song_name}':")
        for i, (song, score) in enumerate(zip(recommended_songs, similarity_scores)):
            song = song.replace('.wav', '')
            print(f"{song.ljust(80)} {score:.6f}")
        print("*******")
    else:
        print(f"Song '{song_name}' not found in the dataset.")

recommend('Radiohead - Creep.wav')

"""Random Forest"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, scaled_data, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Defining the RandomForestRegressor model
forest = RandomForestRegressor(random_state=42)

# Defining the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initializing GridSearchCV
grid_search = GridSearchCV(estimator=forest, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train, y_train)

# Printing the best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Training the model with the best parameters
best_forest = grid_search.best_estimator_
best_forest.fit(X_train, y_train)

# Evaluate on the validation set
val_predictions = best_forest.predict(X_val)
val_mse = mean_squared_error(y_val, val_predictions)
val_mae = mean_absolute_error(y_val, val_predictions)
val_r2 = r2_score(y_val, val_predictions)

print(f"Validation MSE: {val_mse:.4f}")
print(f"Validation MAE: {val_mae:.4f}")
print(f"Validation R²: {val_r2:.4f}")

def recommend(song_name, num_recommendations=5):
    if song_name in data['song_name'].values:
        song_index = data[data['song_name'] == song_name].index[0]
        song_features = numeric_data.iloc[song_index].values.reshape(1, -1)
        scaled_song_features = scaler.transform(song_features)

        # Predict similar songs
        predictions = best_forest.predict(scaled_song_features)

        # Calculate cosine similarity
        similarities = cosine_similarity(scaled_data, predictions.reshape(1, -1)).flatten()
        recommended_indices = similarities.argsort()[-(num_recommendations+1):-1][::-1]

        # Normalize similarity scores
        similarity_scores = similarities[recommended_indices]

        # Get recommended song names
        recommended_songs = data.iloc[recommended_indices]['song_name'].values

        song_name = song_name.replace('.wav', '')
        # Display the recommended songs and their similarity scores
        print(f"*******\nSimilar songs to '{song_name}':")
        for i, (song, score) in enumerate(zip(recommended_songs, similarity_scores)):
            song = song.replace('.wav', '')
            print(f"{song.ljust(80)} {score:.6f}")
        print("*******")
    else:
        print(f"Song '{song_name}' not found in the dataset.")

recommend('Radiohead - Creep.wav')

"""Support Vector Machine"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVR
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Assume the first column is the target variable for simplicity
target_column = numeric_data.columns[0]
y = numeric_data[target_column].values
X = numeric_data.drop(columns=[target_column])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(X)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Defining the SVR model
svm = SVR()

# Defining the parameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto'],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']
}

# Initializing GridSearchCV
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)
grid_search.fit(X_train, y_train)

# Printing the best parameters
print(f"Best parameters: {grid_search.best_params_}")

# Training the model with the best parameters
best_svm = grid_search.best_estimator_
best_svm.fit(X_train, y_train)

def recommend(song_name, num_recommendations=5):
    if song_name in data['song_name'].values:
        song_index = data[data['song_name'] == song_name].index[0]
        song_features = X.iloc[song_index].values.reshape(1, -1)
        scaled_song_features = scaler.transform(song_features)

        # Calculate cosine similarity
        similarities = cosine_similarity(scaled_data, scaled_song_features).flatten()
        recommended_indices = similarities.argsort()[-(num_recommendations+1):-1][::-1]

        # Normalize similarity scores
        similarity_scores = similarities[recommended_indices]

        # Get recommended song names
        recommended_songs = data.iloc[recommended_indices]['song_name'].values

        song_name = song_name.replace('.wav', '')
        # Display the recommended songs and their similarity scores
        print(f"*******\nSimilar songs to '{song_name}':")
        for i, (song, score) in enumerate(zip(recommended_songs, similarity_scores)):
            song = song.replace('.wav', '')
            print(f"{song.ljust(80)} {score:.6f}")
        print("*******")
    else:
        print(f"Song '{song_name}' not found in the dataset.")

recommend('Radiohead - Creep.wav')

"""Comparing Models:"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense
from scikeras.wrappers import KerasRegressor

# Load the dataset
data = pd.read_csv('/content/drive/Shareddrives/Machine_Learning_Project_Drive/Audio_Features/all_audio_features_modified.csv')

# Drop non-numeric columns and set song_name as index
numeric_data = data.drop(columns=['song_name', 'genre'])

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(scaled_data, scaled_data[:, 0], test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Neural Network Model
def create_model(optimizer='adam', init='normal', neurons1=64, neurons2=64, neurons3=64):
    model = Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(neurons1, activation='relu', kernel_initializer=init),
        Dense(neurons2, activation='relu', kernel_initializer=init),
        Dense(neurons3, activation='relu', kernel_initializer=init),
        Dense(1, activation='linear')
    ])
    model.compile(optimizer=optimizer, loss='mse')
    return model

model = KerasRegressor(model=create_model, verbose=0, epochs=150, batch_size=16)
model.fit(X_train, y_train, validation_data=(X_val, y_val))

nn_predictions = model.predict(X_test)
nn_mse = mean_squared_error(y_test, nn_predictions)
nn_mae = mean_absolute_error(y_test, nn_predictions)
nn_r2 = r2_score(y_test, nn_predictions)

print(f"Neural Network MSE: {nn_mse:.4f}")
print(f"Neural Network MAE: {nn_mae:.4f}")
print(f"Neural Network R²: {nn_r2:.4f}")

# KNN Model
knn = KNeighborsRegressor(algorithm='auto', n_neighbors=7, weights='distance')
knn.fit(X_train, y_train)
knn_scores = cross_val_score(knn, X_val, y_val, cv=3, scoring='neg_mean_squared_error')
knn_predictions = knn.predict(X_test)
knn_mse = mean_squared_error(y_test, knn_predictions)
knn_mae = mean_absolute_error(y_test, knn_predictions)
knn_r2 = r2_score(y_test, knn_predictions)

print(f"KNN MSE: {knn_mse:.4f}")
print(f"KNN MAE: {knn_mae:.4f}")
print(f"KNN R²: {knn_r2:.4f}")

# Decision Tree Model
dt = DecisionTreeRegressor(max_depth=20, min_samples_leaf=8, min_samples_split=20)
dt.fit(X_train, y_train)
dt_scores = cross_val_score(dt, X_val, y_val, cv=3, scoring='neg_mean_squared_error')
dt_predictions = dt.predict(X_test)
dt_mse = mean_squared_error(y_test, dt_predictions)
dt_mae = mean_absolute_error(y_test, dt_predictions)
dt_r2 = r2_score(y_test, dt_predictions)

print(f"Decision Tree MSE: {dt_mse:.4f}")
print(f"Decision Tree MAE: {dt_mae:.4f}")
print(f"Decision Tree R²: {dt_r2:.4f}")

# Random Forest Model
forest = RandomForestRegressor(bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=42)
forest.fit(X_train, y_train)
rf_scores = cross_val_score(forest, X_val, y_val, cv=3, scoring='neg_mean_squared_error')
rf_predictions = forest.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

print(f"Random Forest MSE: {rf_mse:.4f}")
print(f"Random Forest MAE: {rf_mae:.4f}")
print(f"Random Forest R²: {rf_r2:.4f}")

# SVM Model
svm = SVR(C=100, gamma='scale', kernel='linear')
svm.fit(X_train, y_train)
svm_scores = cross_val_score(svm, X_val, y_val, cv=3, scoring='neg_mean_squared_error')
svm_predictions = svm.predict(X_test)
svm_mse = mean_squared_error(y_test, svm_predictions)
svm_mae = mean_absolute_error(y_test, svm_predictions)
svm_r2 = r2_score(y_test, svm_predictions)

print(f"SVM MSE: {svm_mse:.4f}")
print(f"SVM MAE: {svm_mae:.4f}")
print(f"SVM R²: {svm_r2:.4f}")